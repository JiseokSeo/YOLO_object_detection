{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cu0Ls8tLMJI",
        "outputId": "9b66cdef-1b1d-41fd-de5c-1e654d6de615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JwaZFDyeygG5"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations as A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgKdw3_0R-VQ",
        "outputId": "6861fa96-7c83-4577-b2ce-51514d5e0ea2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 üöÄ v7.0-120-g3e55763 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete ‚úÖ (4 CPUs, 25.5 GB RAM, 25.5/166.8 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# YOLO v5\n",
        "\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils # yoloÏùò utilsÏûÖÎãàÎã§\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG6p6gIvS5LL"
      },
      "outputs": [],
      "source": [
        "# Í∞úÏù∏ Íµ¨Í∏ÄÎìúÎùºÏù¥Î∏åÏóê Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÎÑ£Ïñ¥ÎÜìÏïòÏäµÎãàÎã§\n",
        "# Ïù¥Í±∏ ÏΩîÎû© Í≤ΩÎ°úÎ°ú ÏïïÏ∂ïÌï¥Ï†úÌï©ÎãàÎã§\n",
        "\n",
        "!unzip /content/drive/MyDrive/data2.zip -d /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hRq280Y6g4-m"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/yolov5/data/labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "41XuftCM2_h4"
      },
      "outputs": [],
      "source": [
        "def get_YOLO_txt(all_json_paths):\n",
        "  for json_path in all_json_paths:\n",
        "    with open(json_path) as f:\n",
        "      data = json.load(f)\n",
        "      \n",
        "      image_name = re.sub('.json', '.txt', json_path).split('/')[-1]\n",
        "\n",
        "      dw, dh = map(float, data['FILE'][0]['RESOLUTION'].split('*'))\n",
        "      dw, dh = 1/dw, 1/dh\n",
        "\n",
        "      line = ''\n",
        "      for item in data['FILE'][0]['ITEMS']:\n",
        "        x1, y1, x2, y2 = map(float, item['BOX'].split(','))\n",
        "\n",
        "        x = x1 + x2 / 2.0\n",
        "        y = y1 + y2 / 2.0\n",
        "        w = x2\n",
        "        h = y2\n",
        "\n",
        "        x_center = x * dw\n",
        "        width = w * dw\n",
        "        y_center = y * dh\n",
        "        height = h * dh\n",
        "\n",
        "        classes = 0\n",
        "\n",
        "        line += str(classes) + ' ' + ' '.join(map(str, [x_center, y_center, width, height])) + '\\n'\n",
        "\n",
        "      line = line.rstrip()\n",
        "      with open(os.path.join('/content/yolov5/data/labels', image_name), 'a') as f:\n",
        "        f.write(line + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbCVOJ4VaQow",
        "outputId": "e48ebe3c-3455-479a-d064-e54502b5b3e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1636"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "json_paths = glob.glob('/content/data2/label/*.json')\n",
        "len(json_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mHPJ7hxD_7n",
        "outputId": "c20de231-af99-4830-fa63-52a34dd6de57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1539"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "real = []\n",
        "for path in json_paths:\n",
        "  if path.startswith('/content/data2/label/A01_B01_C04_D02_1201') or path.startswith('/content/data2/label/A01_B01_C04_D03_1122'):\n",
        "    None\n",
        "  else:\n",
        "    real.append(path)\n",
        "\n",
        "json_paths = real\n",
        "len(json_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IB16qasn0FEV"
      },
      "outputs": [],
      "source": [
        "get_YOLO_txt(json_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-LtPp1iPaOgh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "image_path = glob.glob('/content/data2/image/*.jpg')\n",
        "destination_dir = '/content/yolov5/data/images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MWY63OMdK3lM"
      },
      "outputs": [],
      "source": [
        "# ÏõêÎûò Ìè¥Îçî ÎÇ¥Ïùò Î™®Îì† ÌååÏùºÏùÑ Í∞ÄÏ†∏ÏôÄÏÑú Î™©Ï†Å Ìè¥ÎçîÎ°ú Ïù¥Îèô\n",
        "for path in image_path:\n",
        "  if path.startswith('/content/data2/image/A01_B01_C04_D02_1201') or path.startswith('/content/data2/image/A01_B01_C04_D03_1122'):\n",
        "    None\n",
        "  else:\n",
        "    shutil.move(path, destination_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8qR9bbmJwHx",
        "outputId": "88f5bf5e-ecf5-4f1c-b854-0d2f92d08780"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1540"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(os.listdir('/content/yolov5/data/images'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMoQya4SJ-ds",
        "outputId": "bb8a51d2-ce32-4bb9-f108-1e5fb0ec8fdb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1539"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(os.listdir('/content/yolov5/data/labels'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0c5Uw7NZbEvT"
      },
      "outputs": [],
      "source": [
        "# train, valid\n",
        "images_paths = glob.glob('/content/yolov5/data/images/*.jpg')\n",
        "\n",
        "train_path, valid_path = train_test_split(images_paths,\n",
        "                                          test_size=0.2,\n",
        "                                          random_state=777,\n",
        "                                          shuffle=True)\n",
        "\n",
        "with open('/content/train.txt', 'w') as f:\n",
        "  f.write('\\n'.join(train_path) + '\\n')\n",
        "\n",
        "with open('/content/valid.txt', 'w') as f:\n",
        "  f.write('\\n'.join(valid_path) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Eda09TKPDdev"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "# train.txt, val.txt ÌååÏùº Í≤ΩÎ°ú\n",
        "train_file = '/content/train.txt'\n",
        "val_file = '/content/valid.txt'\n",
        "\n",
        "# ÌÅ¥ÎûòÏä§ Ïù¥Î¶Ñ Î¶¨Ïä§Ìä∏\n",
        "classes = ['ÌôîÎ¨ºÏ∞®']\n",
        "\n",
        "# data.yaml ÌååÏùº Í≤ΩÎ°ú\n",
        "data_file = '/content/data.yaml'\n",
        "\n",
        "# data.yaml ÌååÏùº ÏÉùÏÑ±\n",
        "data = dict(\n",
        "    train=train_file,\n",
        "    val=val_file,\n",
        "    nc=len(classes),\n",
        "    names=classes\n",
        ")\n",
        "\n",
        "# ÌïúÍ∏Ä Î¨∏ÏûêÏó¥ ÏßÄÏõêÏùÑ ÏúÑÌïú ÏÑ§Ï†ï\n",
        "yaml.add_representer(str, lambda dumper, data: dumper.represent_scalar('tag:yaml.org,2002:str', data, style='\"'))\n",
        "\n",
        "with open(data_file, 'w', encoding='UTF-8') as f:\n",
        "    yaml.dump(data, f, allow_unicode=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "l77k2mHbCY16"
      },
      "outputs": [],
      "source": [
        "!cd /content/yolov5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGnkt_KkCoqd",
        "outputId": "1068b861-7c76-4c58-899e-cd28aa9a2f2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5m.pt, cfg=/content/yolov5/models/yolov5m.yaml, data=/content/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=20, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=/content/drive/MyDrive/fin_20_3, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
            "YOLOv5 üöÄ v7.0-120-g3e55763 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/drive/MyDrive', view at http://localhost:6006/\n",
            "2023-03-20 09:01:37.033475: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-20 09:01:37.903191: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-20 09:01:37.903315: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-03-20 09:01:37.903338: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
            "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
            "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
            "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
            "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
            "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
            "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
            "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
            "  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
            "  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
            " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
            " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
            " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
            " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n",
            " 24      [17, 20, 23]  1     24246  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
            "YOLOv5m summary: 291 layers, 20871318 parameters, 20871318 gradients, 48.2 GFLOPs\n",
            "\n",
            "Transferred 474/481 items from yolov5m.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 79 weight(decay=0.0), 82 weight(decay=0.0005), 82 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mHorizontalFlip(p=0.5), Rotate(p=0.2, limit=(-15, 15), interpolation=1, border_mode=4, value=None, mask_value=None, method='largest_box', crop_border=False), Blur(p=0.05, blur_limit=(3, 7)), ToGray(p=0.01), RandomBrightnessContrast(p=0.05, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True), ImageCompression(p=0.1, quality_lower=75, quality_upper=100, compression_type=0)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/train.cache... 1231 images, 0 backgrounds, 0 corrupt: 100% 1231/1231 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/valid.cache... 308 images, 0 backgrounds, 0 corrupt: 100% 308/308 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.75 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
            "Plotting labels to /content/drive/MyDrive/fin_20_3/labels.jpg... \n",
            "Image sizes 416 train, 416 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/fin_20_3\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/19      2.92G    0.08088    0.02196          0         34        416: 100% 77/77 [00:53<00:00,  1.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:06<00:00,  1.65it/s]\n",
            "                   all        308        309      0.447       0.54      0.538      0.186\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/19      3.05G    0.05416    0.01816          0         18        416: 100% 77/77 [00:51<00:00,  1.49it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:06<00:00,  1.65it/s]\n",
            "                   all        308        309       0.56      0.735      0.634      0.212\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/19      3.05G    0.05019    0.01551          0         31        416: 100% 77/77 [00:51<00:00,  1.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:04<00:00,  2.04it/s]\n",
            "                   all        308        309      0.733      0.825       0.86      0.475\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/19      3.05G    0.04362    0.01199          0         25        416: 100% 77/77 [00:51<00:00,  1.49it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:04<00:00,  2.10it/s]\n",
            "                   all        308        309      0.911      0.964      0.962      0.556\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/19      3.05G    0.03847   0.009708          0         28        416: 100% 77/77 [00:53<00:00,  1.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:04<00:00,  2.27it/s]\n",
            "                   all        308        309      0.892      0.971      0.941      0.552\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/19      3.05G     0.0339    0.00868          0         39        416: 100% 77/77 [00:54<00:00,  1.41it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:04<00:00,  2.44it/s]\n",
            "                   all        308        309      0.965       0.99      0.984      0.603\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/19      3.05G    0.02967   0.007722          0         33        416: 100% 77/77 [00:52<00:00,  1.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:05<00:00,  1.79it/s]\n",
            "                   all        308        309      0.972      0.995      0.988      0.744\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/19      3.05G    0.02703   0.007302          0         29        416: 100% 77/77 [00:51<00:00,  1.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:06<00:00,  1.52it/s]\n",
            "                   all        308        309      0.971      0.997      0.989      0.776\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/19      3.05G    0.02507   0.006819          0         28        416: 100% 77/77 [00:52<00:00,  1.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:04<00:00,  2.11it/s]\n",
            "                   all        308        309      0.979      0.994       0.99      0.743\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/19      3.05G    0.02338   0.006438          0         28        416: 100% 77/77 [00:53<00:00,  1.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:03<00:00,  2.53it/s]\n",
            "                   all        308        309      0.981      0.998      0.989      0.861\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/19      3.05G    0.02181   0.006433          0         29        416: 100% 77/77 [00:52<00:00,  1.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:04<00:00,  2.33it/s]\n",
            "                   all        308        309      0.983      0.997      0.991      0.764\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/19      3.05G    0.02017   0.006025          0         35        416: 100% 77/77 [00:54<00:00,  1.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:04<00:00,  2.08it/s]\n",
            "                   all        308        309       0.98      0.997      0.988       0.85\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/19      3.05G    0.01889   0.005785          0         36        416: 100% 77/77 [00:54<00:00,  1.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:04<00:00,  2.05it/s]\n",
            "                   all        308        309       0.98      0.997      0.992      0.862\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/19      3.05G    0.01767   0.005565          0         21        416: 100% 77/77 [00:53<00:00,  1.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:05<00:00,  1.75it/s]\n",
            "                   all        308        309      0.986      0.997      0.992      0.849\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      14/19      3.05G    0.01651   0.005349          0         25        416: 100% 77/77 [00:53<00:00,  1.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:05<00:00,  1.75it/s]\n",
            "                   all        308        309      0.985      0.997      0.992      0.894\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      15/19      3.05G    0.01504   0.005123          0         26        416: 100% 77/77 [00:52<00:00,  1.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:05<00:00,  1.83it/s]\n",
            "                   all        308        309      0.986      0.997      0.993      0.894\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      16/19      3.05G    0.01428   0.004922          0         30        416: 100% 77/77 [00:52<00:00,  1.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:04<00:00,  2.05it/s]\n",
            "                   all        308        309      0.986      0.997      0.993      0.907\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      17/19      3.05G    0.01339   0.004758          0         28        416: 100% 77/77 [00:52<00:00,  1.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:04<00:00,  2.31it/s]\n",
            "                   all        308        309      0.989      0.997      0.994      0.928\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      18/19      3.05G     0.0121   0.004639          0         33        416: 100% 77/77 [00:52<00:00,  1.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:04<00:00,  2.08it/s]\n",
            "                   all        308        309      0.987      0.999      0.993      0.932\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      19/19      3.05G    0.01145   0.004602          0         31        416: 100% 77/77 [00:53<00:00,  1.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:04<00:00,  2.38it/s]\n",
            "                   all        308        309       0.99      0.997      0.994       0.94\n",
            "\n",
            "20 epochs completed in 0.335 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/fin_20_3/weights/last.pt, 42.0MB\n",
            "Optimizer stripped from /content/drive/MyDrive/fin_20_3/weights/best.pt, 42.0MB\n",
            "\n",
            "Validating /content/drive/MyDrive/fin_20_3/weights/best.pt...\n",
            "Fusing layers... \n",
            "YOLOv5m summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 10/10 [00:05<00:00,  1.99it/s]\n",
            "                   all        308        309       0.99      0.997      0.994      0.941\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/fin_20_3\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python train.py --img 416 --batch 16 --epochs 20 --data /content/data.yaml --cfg /content/yolov5/models/yolov5m.yaml --weights yolov5m.pt --name /content/drive/MyDrive/fin_20_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d_5DURozzAVK"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import albumentations\n",
        "\n",
        "def detection(input_video, output_video, model):\n",
        "    cap = cv2.VideoCapture(input_video)\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
        "\n",
        "    with tqdm(total=total_frames, desc=\"Processing\", unit=\"frame\") as pbar:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            results = model(frame)\n",
        "            result_frame = results.render()[0]\n",
        "            out.write(result_frame)\n",
        "            pbar.update(1)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2P6_FAazBre",
        "outputId": "2260e2b8-4c71-4c01-9406-afbddb5fc9fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 üöÄ 2023-3-20 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5m summary: 212 layers, 20852934 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "model = torch.hub.load('ultralytics/yolov5', 'custom', '/content/drive/MyDrive/fin_20_3/weights/best.pt')\n",
        "input_video = '/content/sample4.mp4'\n",
        "output_video = '/content/20_re4.mp4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8Zx7IOnziQk",
        "outputId": "8039b3c8-46ad-4133-dd08-4f44a3645232"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 915/915 [00:43<00:00, 21.28frame/s]\n"
          ]
        }
      ],
      "source": [
        "detection(model = model,\n",
        "          input_video = input_video,\n",
        "          output_video = output_video)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
